# main.py ‚Äî AI Halyava Bot (Py 3.13.4, aiogram 3.22)
# One-file, long-polling; SQLite + APScheduler; —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–±–æ—Ä –ø—Ä–æ–º–æ + –ø–æ–∏—Å–∫ –ø–æ –º–∞–≥–∞–∑–∏–Ω—É
import os, sqlite3, datetime, hashlib, json, asyncio, logging, re, time
from typing import Optional, List, Dict
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import feedparser
from zoneinfo import ZoneInfo

from aiogram import Bot, Dispatcher, Router
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.filters import Command
from aiogram.types import Message, LinkPreviewOptions
from apscheduler.schedulers.asyncio import AsyncIOScheduler

# ======== LOGGING ========
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("halyava")

# ======== CONFIG ========
BOT_TOKEN = os.environ.get("BOT_TOKEN", "")
TRIAL_DAYS = int(os.environ.get("TRIAL_DAYS", "3"))
MONTHLY_PRICE_RUB = int(os.environ.get("MONTHLY_PRICE_RUB", "249"))
DB_PATH = os.environ.get("DB_PATH", "/data/halyava.db")
TIMEZONE = os.environ.get("TIMEZONE", "Europe/Moscow")

# –ï—Å–ª–∏ –≤ ENV –Ω–µ—Ç STORES_JSON ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç –Ω–∏–∂–µ (–≤–∞–ª–∏–¥–Ω—ã–π JSON)
STORES_JSON = os.environ.get("STORES_JSON")

DEFAULT_STORES_JSON = """
{
  "stores": [
    { "type":"auto","store":"ozon","category":"–∞–∫—Ü–∏–∏","url":"https://www.ozon.ru/info/actions/" },
    { "type":"auto","store":"wb","category":"–∞–∫—Ü–∏–∏","url":"https://www.wildberries.ru/promotions" },
    { "type":"auto","store":"yandexmarket","category":"–∞–∫—Ü–∏–∏","url":"https://market.yandex.ru/specials" },
    { "type":"auto","store":"sbermegamarket","category":"–∞–∫—Ü–∏–∏","url":"https://sbermegamarket.ru/actions/" },

    { "type":"auto","store":"mvideo","category":"–∞–∫—Ü–∏–∏","url":"https://www.mvideo.ru/promo" },
    { "type":"auto","store":"eldorado","category":"–∞–∫—Ü–∏–∏","url":"https://www.eldorado.ru/promo/" },
    { "type":"auto","store":"dns","category":"–∞–∫—Ü–∏–∏","url":"https://www.dns-shop.ru/actions/" },
    { "type":"auto","store":"citilink","category":"–∞–∫—Ü–∏–∏","url":"https://www.citilink.ru/promo/" },
    { "type":"auto","store":"technopark","category":"–∞–∫—Ü–∏–∏","url":"https://www.technopark.ru/promo/" },

    { "type":"auto","store":"lamoda","category":"–∞–∫—Ü–∏–∏","url":"https://www.lamoda.ru/promo/" },
    { "type":"auto","store":"sportmaster","category":"–∞–∫—Ü–∏–∏","url":"https://www.sportmaster.ru/actions/" },

    { "type":"auto","store":"letual","category":"–∞–∫—Ü–∏–∏","url":"https://www.letu.ru/promo" },
    { "type":"auto","store":"rivegauche","category":"–∞–∫—Ü–∏–∏","url":"https://www.rivegauche.ru/promo" },

    { "type":"auto","store":"apteka","category":"–∞–∫—Ü–∏–∏","url":"https://apteka.ru/discounts" },
    { "type":"auto","store":"rigla","category":"–∞–∫—Ü–∏–∏","url":"https://www.rigla.ru/actions/" },
    { "type":"auto","store":"aptekamos","category":"–∞–∫—Ü–∏–∏","url":"https://www.apteka-mos.ru/actions/" },

    { "type":"auto","store":"vkusvill","category":"–∞–∫—Ü–∏–∏","url":"https://vkusvill.ru/akcii/" },
    { "type":"auto","store":"perekrestok","category":"–∞–∫—Ü–∏–∏","url":"https://www.perekrestok.ru/cat/akcii" },
    { "type":"auto","store":"magnit","category":"–∞–∫—Ü–∏–∏","url":"https://magnit.ru/promo/" },
    { "type":"auto","store":"lenta","category":"–∞–∫—Ü–∏–∏","url":"https://lenta.com/promo/" },
    { "type":"auto","store":"auchan","category":"–∞–∫—Ü–∏–∏","url":"https://www.auchan.ru/promo/" },
    { "type":"auto","store":"okey","category":"–∞–∫—Ü–∏–∏","url":"https://www.okmarket.ru/actions/" },
    { "type":"auto","store":"metro","category":"–∞–∫—Ü–∏–∏","url":"https://www.metro-cc.ru/promo" },
    { "type":"auto","store":"sbermarket","category":"–∞–∫—Ü–∏–∏","url":"https://sbermarket.ru/actions" },

    { "type":"auto","store":"deliveryclub","category":"–∞–∫—Ü–∏–∏","url":"https://delivery-club.ru/special" }
  ]
}
"""

# –ê–ª–∏–∞—Å—ã –¥–ª—è —É–ø—Ä–æ—â—ë–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–∞–≥–∞–∑–∏–Ω–∞ (—é–∑–µ—Ä –≤–≤–æ–¥–∏—Ç ¬´–æ–∑–æ–Ω¬ª ‚Üí ozon)
ALIASES: Dict[str, str] = {
    "ozon": "ozon", "–æ–∑–æ–Ω": "ozon",
    "wildberries": "wb", "–≤–±": "wb", "wb": "wb", "–≤–∞–π–ª–¥–±–µ—Ä—Ä–∏–∑": "wb", "–≤–∞–π–ª–¥–±–µ—Ä—Ä–∏—Å": "wb",
    "yandexmarket": "yandexmarket", "—è–Ω–¥–µ–∫—Å–º–∞—Ä–∫–µ—Ç": "yandexmarket", "–º–∞—Ä–∫–µ—Ç": "yandexmarket", "ym": "yandexmarket",
    "sbermegamarket": "sbermegamarket", "—Å–±–µ—Ä–º–µ–≥–∞–º–∞—Ä–∫–µ—Ç": "sbermegamarket", "—Å–º–º": "sbermegamarket",

    "mvideo": "mvideo", "–º–≤–∏–¥–µ–æ": "mvideo",
    "eldorado": "eldorado", "—ç–ª—å–¥–æ—Ä–∞–¥–æ": "eldorado",
    "dns": "dns", "–¥–Ω—Å": "dns",
    "citilink": "citilink", "—Å–∏—Ç–∏–ª–∏–Ω–∫": "citilink",
    "technopark": "technopark", "—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫": "technopark",

    "lamoda": "lamoda", "–ª–∞–º–æ–¥–∞": "lamoda",
    "sportmaster": "sportmaster", "—Å–ø–æ—Ä—Ç–º–∞—Å—Ç–µ—Ä": "sportmaster",

    "letual": "letual", "–ª–µ—Ç—É–∞–ª—å": "letual",
    "rivegauche": "rivegauche", "—Ä–∏–≤ –≥–æ—à": "rivegauche", "—Ä–∏–≤–≥–æ—à": "rivegauche",

    "apteka": "apteka", "–∞–ø—Ç–µ–∫–∞ —Ä—É": "apteka", "–∞–ø—Ç–µ–∫–∞.—Ä—É": "apteka",
    "rigla": "rigla", "—Ä–∏–≥–ª–∞": "rigla",
    "aptekamos": "aptekamos", "–∞–ø—Ç–µ–∫–∞ –º–æ—Å": "aptekamos",

    "vkusvill": "vkusvill", "–≤–∫—É—Å–≤–∏–ª–ª": "vkusvill",
    "perekrestok": "perekrestok", "–ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–æ–∫": "perekrestok",
    "magnit": "magnit", "–º–∞–≥–Ω–∏—Ç": "magnit",
    "lenta": "lenta", "–ª–µ–Ω—Ç–∞": "lenta",
    "auchan": "auchan", "–∞—à–∞–Ω": "auchan",
    "okey": "okey", "–æ–∫–µ–π": "okey",
    "metro": "metro", "–º–µ—Ç—Ä–æ": "metro",
    "sbermarket": "sbermarket", "—Å–±–µ—Ä–º–∞—Ä–∫–µ—Ç": "sbermarket",

    "deliveryclub": "deliveryclub", "–¥–æ—Å—Ç–∞–≤–∫–ª–∞–±": "deliveryclub", "–¥–µ–ª–∏–≤–µ—Ä–∏": "deliveryclub",
}

# –ü—Ä–æ–º–æ–∫–æ–¥—ã –¥–ª—è MVP: ENV PROMO_CODES="VIP,TEST1"
PROMO_CODES = {c.strip() for c in os.environ.get("PROMO_CODES", "").split(",") if c.strip()}

# ======== DB LAYER ========
def connect() -> sqlite3.Connection:
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    conn = sqlite3.connect(DB_PATH, timeout=60.0, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA synchronous=NORMAL;")
    conn.execute("PRAGMA busy_timeout=60000;")
    return conn

def init_db():
    with connect() as conn:
        conn.executescript("""
        CREATE TABLE IF NOT EXISTS users(
          user_id INTEGER PRIMARY KEY,
          username TEXT,
          created_at TEXT DEFAULT CURRENT_TIMESTAMP
        );
        CREATE TABLE IF NOT EXISTS subscriptions(
          user_id INTEGER PRIMARY KEY,
          status TEXT,
          until TEXT,
          plan TEXT,
          updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        );
        CREATE TABLE IF NOT EXISTS deals(
          id INTEGER PRIMARY KEY AUTOINCREMENT,
          store_slug TEXT,
          category TEXT,
          title TEXT,
          description TEXT,
          url TEXT,
          price_old REAL,
          price_new REAL,
          cashback REAL,
          coupon_code TEXT,
          start_at TEXT,
          end_at TEXT,
          created_at TEXT DEFAULT CURRENT_TIMESTAMP,
          source TEXT,
          hash TEXT UNIQUE,
          score REAL DEFAULT 0
        );
        CREATE INDEX IF NOT EXISTS idx_deals_store ON deals(store_slug);
        CREATE INDEX IF NOT EXISTS idx_deals_cat ON deals(category);
        CREATE INDEX IF NOT EXISTS idx_deals_end ON deals(end_at);
        """)

def now_utc_iso() -> str:
    return datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None, microsecond=0).isoformat()

def upsert_user(user_id:int, username:str=""):
    with connect() as conn:
        conn.execute("INSERT OR IGNORE INTO users(user_id, username) VALUES(?,?)", (user_id, username or ""))

def get_sub(user_id:int) -> Optional[dict]:
    with connect() as conn:
        r = conn.execute("SELECT status, until FROM subscriptions WHERE user_id=?", (user_id,)).fetchone()
    return dict(r) if r else None

def set_sub(user_id:int, status:str, until_iso:str, plan:str="monthly"):
    with connect() as conn:
        conn.execute("""
            INSERT INTO subscriptions(user_id,status,until,plan,updated_at)
            VALUES(?,?,?,?,?)
            ON CONFLICT(user_id) DO UPDATE
            SET status=excluded.status, until=excluded.until, updated_at=excluded.updated_at
        """, (user_id, status, until_iso, plan, now_utc_iso()))

def sub_active(user_id:int) -> bool:
    sub = get_sub(user_id)
    if not sub: return False
    try:
        return datetime.datetime.fromisoformat(sub["until"]) >= datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None)
    except Exception:
        return False

def grant_trial(user_id:int, days:int=TRIAL_DAYS) -> str:
    until = (datetime.datetime.now(datetime.timezone utc:=datetime.timezone.utc).replace(tzinfo=None) + datetime.timedelta(days=days)).replace(microsecond=0).isoformat()
    set_sub(user_id, "trial", until)
    return until
# ‚Üë –í Python 3.13 –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è walrus? –ß—Ç–æ–±—ã –Ω–µ —Ä–∏—Å–∫–æ–≤–∞—Ç—å, –ø–µ—Ä–µ–ø–∏—à–µ–º –±–µ–∑ –Ω–µ–≥–æ:
def grant_trial(user_id:int, days:int=TRIAL_DAYS) -> str:  # re-define (safe)
    until = (datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None) + datetime.timedelta(days=days)).replace(microsecond=0).isoformat()
    set_sub(user_id, "trial", until)
    return until

def grant_month(user_id:int, months:int=1) -> str:
    until = (datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None) + datetime.timedelta(days=30*months)).replace(microsecond=0).isoformat()
    set_sub(user_id, "active", until)
    return until

def _insert_with_retry(conn: sqlite3.Connection, sql: str, params: tuple, retries:int=3, delay:float=0.2):
    for i in range(retries):
        try:
            conn.execute(sql, params)
            return
        except sqlite3.OperationalError as e:
            if "locked" in str(e).lower() and i < retries-1:
                time.sleep(delay * (i+1))
                continue
            raise

def put_deal(d:dict) -> bool:
    h = hashlib.sha256((d.get("url","") + d.get("title","")).encode("utf-8")).hexdigest()
    d["hash"] = h
    keys = ["store_slug","category","title","description","url","price_old","price_new","cashback","coupon_code","start_at","end_at","source","score","hash"]
    vals = [d.get(k) for k in keys]
    try:
        with connect() as conn:
            _insert_with_retry(conn,
                f"INSERT INTO deals({','.join(keys)}) VALUES({','.join(['?']*len(keys))})",
                tuple(vals)
            )
        return True
    except sqlite3.IntegrityError:
        return False
    except sqlite3.OperationalError as e:
        log.warning(f"[DB] put_deal OperationalError: {e}")
        return False

def search_deals(store:Optional[str], limit:int=10) -> List[dict]:
    q = "SELECT * FROM deals WHERE 1=1"
    args = []
    if store:
        q += " AND store_slug=?"
        args.append(store)
    q += " AND (end_at IS NULL OR end_at>=?)"
    args.append(now_utc_iso())
    q += " ORDER BY score DESC, (end_at IS NULL) ASC, end_at ASC, created_at DESC LIMIT ?"
    args.append(limit)
    with connect() as conn:
        rows = conn.execute(q, tuple(args)).fetchall()
    return [dict(r) for r in rows]

def cleanup_old(ttl_days: int = 14):
    now = now_utc_iso()
    older_than = (datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None) - datetime.timedelta(days=ttl_days)).replace(microsecond=0).isoformat()
    with connect() as conn:
        deleted = conn.execute(
            "DELETE FROM deals WHERE (end_at IS NOT NULL AND end_at < ?) OR (created_at < ?)",
            (now, older_than),
        ).rowcount
    log.info(f"[CLEANUP] deleted={deleted}")

# ======== SCRAPERS ========
HEADERS = {
    "User-Agent":"Mozilla/5.0 (compatible; HalyavaBot/1.0; +https://t.me/)",
    "Accept-Language":"ru-RU,ru;q=0.9"
}
KEYWORDS = ["–∞–∫—Ü–∏", "—Å–∫–∏–¥", "–∫—É–ø–æ–Ω", "–ø—Ä–æ–º–æ", "—Ä–∞—Å–ø—Ä–æ–¥", "sale", "%", "–≤—ã–≥–æ–¥", "–±–æ–Ω—É—Å", "—Å–ø–µ—Ü", "–ø—Ä–æ–º–æ–∫–æ–¥", "coupon"]

CLASS_HINTS = re.compile(r"(promo|action|sale|discount|deal|offer|bonus|coupon|kupon|akci|skid|–≤—ã–≥–æ–¥|–∞–∫—Ü–∏|—Å–∫–∏–¥|—Å–ø–µ—Ü|—Ä–∞—Å–ø—Ä–æ–¥)", re.I)

def _extract_best_title(a_tag, soup) -> str:
    text = " ".join((a_tag.get_text() or "").split())
    if len(text) >= 8 and not re.search(r"–ø–æ–¥—Ä–æ–±–Ω–µ–µ|—É–∑–Ω–∞—Ç—å|—á–∏—Ç–∞—Ç—å|more", text, re.I):
        return text
    # –∏—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞–≤–µ—Ä—Ö
    for parent in a_tag.parents:
        if not hasattr(parent, "get_text"): break
        h = parent.find(["h1","h2","h3","h4","strong","b"])
        if h:
            t = " ".join(h.get_text().split())
            if t and len(t) >= 6:
                return t
        if getattr(parent, "attrs", None):
            cl = " ".join([parent.get("class",""), parent.get("id","")]) if isinstance(parent.get("class",""), str) else " ".join(parent.get("class",[]) or [])
            if CLASS_HINTS.search(cl):
                t = " ".join(parent.get_text().split())
                t = re.sub(r"\s{2,}", " ", t)
                if t and len(t) >= 12:
                    return t[:180]
    # meta og:title –∫–∞–∫ –∫—Ä–∞–π–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
    og = soup.find("meta", property="og:title")
    if og and og.get("content"):
        return og["content"].strip()
    title = soup.find("title")
    return title.get_text(strip=True) if title else text

def scrape_auto(store, category, url) -> int:
    log.info(f"[SCRAPE][AUTO] {store} {url}")
    try:
        r = requests.get(url, timeout=25, headers=HEADERS)
        r.raise_for_status()
    except Exception as e:
        log.warning(f"[SCRAPE][AUTO] fetch fail {store}: {e}")
        return 0
    soup = BeautifulSoup(r.text, "html.parser")
    added, seen = 0, set()

    # 1) –°–µ–∫—Ü–∏–∏/–∫–∞—Ä—Ç–æ—á–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º-–Ω–∞–º—ë–∫–∞–º
    for box in soup.find_all(True, attrs={"class": CLASS_HINTS}):
        a = box.find("a", href=True) or box
        href = a.get("href") if a and a != box else None
        if href:
            href = urljoin(url, href)
        else:
            href = url
        title = " ".join(box.get_text().split())
        title = re.sub(r"\s{2,}", " ", title)
        if not title or len(title) < 8: 
            continue
        if href in seen:
            continue
        d = dict(store_slug=store, category=category, title=title[:200], description="", url=href,
                 source=url, score=0.95, start_at=None, end_at=None,
                 price_old=None, price_new=None, cashback=None, coupon_code=None)
        if put_deal(d):
            added += 1
            seen.add(href)

    # 2) –°—Å—ã–ª–∫–∏ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for a in soup.find_all("a", href=True)[:3000]:
        href = urljoin(url, a["href"])
        if href in seen or href.startswith(("javascript:", "#")):
            continue
        lowt = (a.get_text() or "").lower()
        near_text = " ".join(a.get_text(separator=" ").split())
        if not any(k in lowt for k in KEYWORDS):
            # –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–¥–Ω—è—Ç—å—Å—è –∫ —Ä–æ–¥–∏—Ç–µ–ª—é –∏ –ø–æ–∏—Å–∫–∞—Ç—å –Ω–∞–º—ë–∫–∏
            par = a.find_parent(True)
            if not par:
                continue
            cl = " ".join(par.get("class", []) if isinstance(par.get("class", []), list) else [par.get("class","")]) + " " + par.get("id","")
            if not (any(k in near_text.lower() for k in KEYWORDS) or CLASS_HINTS.search(cl)):
                continue

        title = _extract_best_title(a, soup)
        if not title or len(title) < 6:
            continue
        d = dict(store_slug=store, category=category, title=title[:200], description="",
                 url=href, source=url, score=0.8, start_at=None, end_at=None,
                 price_old=None, price_new=None, cashback=None, coupon_code=None)
        if put_deal(d):
            added += 1
            seen.add(href)

    log.info(f"[SCRAPE][AUTO] added: {added}")
    return added

def scrape_rss(store, category, url) -> int:
    log.info(f"[SCRAPE][RSS] {store} {url}")
    try:
        feed = feedparser.parse(url)
    except Exception as e:
        log.warning(f"[SCRAPE][RSS] parse fail {store}: {e}")
        return 0
    added = 0
    for e in feed.entries[:300]:
        title = (e.get("title") or "").strip()
        link  = e.get("link") or ""
        summary = (e.get("summary") or "").strip()
        if not title or not link:
            continue
        if not any(k in title.lower() for k in KEYWORDS) and not any(k in summary.lower() for k in KEYWORDS):
            continue
        d = dict(store_slug=store, category=category, title=title[:200], description=summary[:500],
                 url=link, source=url, score=0.7, start_at=None, end_at=None,
                 price_old=None, price_new=None, cashback=None, coupon_code=None)
        if put_deal(d):
            added += 1
    log.info(f"[SCRAPE][RSS] added: {added}")
    return added

def scrape_html_css(store, category, url, item_sel, title_sel, link_sel, desc_sel=None) -> int:
    log.info(f"[SCRAPE][HTML] {store} {url}")
    try:
        r = requests.get(url, timeout=25, headers=HEADERS)
        r.raise_for_status()
    except Exception as e:
        log.warning(f"[SCRAPE][HTML] fetch fail {store}: {e}")
        return 0
    soup = BeautifulSoup(r.text, "html.parser")
    items = soup.select(item_sel)[:300]
    added = 0
    for it in items:
        te = it.select_one(title_sel)
        le = it.select_one(link_sel)
        if not te or not le:
            continue
        title = " ".join(te.get_text().split())
        link  = urljoin(url, le.get("href") or "")
        desc  = ""
        if desc_sel:
            de = it.select_one(desc_sel)
            if de: desc = " ".join(de.get_text().split())
        d = dict(store_slug=store, category=category, title=title[:200], description=desc[:500],
                 url=link, source=url, score=0.9, start_at=None, end_at=None,
                 price_old=None, price_new=None, cashback=None, coupon_code=None)
        if put_deal(d):
            added += 1
    log.info(f"[SCRAPE][HTML] added: {added}")
    return added

def run_all_sources() -> int:
    raw = STORES_JSON or DEFAULT_STORES_JSON
    try:
        conf = json.loads(raw)
    except Exception as e:
        log.error(f"[SCRAPE] bad STORES_JSON: {e}")
        conf = {"stores":[]}
    total = 0
    for s in conf.get("stores", []):
        t = s.get("type")
        try:
            if t == "rss":
                total += scrape_rss(s["store"], s.get("category","–¥—Ä—É–≥–æ–µ"), s["url"])
            elif t == "html_css":
                total += scrape_html_css(
                    s["store"], s.get("category","–¥—Ä—É–≥–æ–µ"),
                    s["url"], s["item_selector"], s["title_selector"], s["link_selector"],
                    s.get("desc_selector")
                )
            else:  # auto / auto_html / None
                total += scrape_auto(s["store"], s.get("category","–¥—Ä—É–≥–æ–µ"), s["url"])
        except Exception:
            log.exception(f"[SCRAPE] error store={s}")
    log.info(f"[SCRAPE] total added: {total}")
    return total

# ======== BOT ========
router = Router()

def fmt_deal(d:dict) -> str:
    price = ""
    if d.get("price_new"):
        price = f"–¶–µ–Ω–∞: {d['price_new']}\n" if not d.get("price_old") else f"–¶–µ–Ω–∞: {d['price_new']} (–±—ã–ª–æ {d['price_old']})\n"
    cb = f"–ö—ç—à–±—ç–∫: {d['cashback']}\n" if d.get("cashback") else ""
    coup = f"–ü—Ä–æ–º–æ–∫–æ–¥: <code>{d['coupon_code']}</code>\n" if d.get("coupon_code") else ""
    deadline = f"–î–µ–¥–ª–∞–π–Ω: {d['end_at']}\n" if d.get("end_at") else ""
    return (
        f"üõí {d['store_slug']} ‚Ä¢ {d.get('category') or '–±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'}\n"
        f"üß© {d['title']}\n"
        f"{price}{cb}{coup}{deadline}"
        f"üîó {d['url']}"
    )

def normalize_store_name(text: str) -> Optional[str]:
    key = text.strip().lower()
    key = re.sub(r"[^a-z–∞-—è0-9]+", "", key)
    return ALIASES.get(key)

@router.message(Command("ping"))
async def cmd_ping(m: Message):
    await m.answer("pong")

@router.message(Command("reload"))
async def cmd_reload(m: Message):
    cnt = run_all_sources()
    await m.answer(f"–û–±–Ω–æ–≤–∏–ª –∏—Å—Ç–æ—á–Ω–∏–∫–∏. –ù–æ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π: {cnt}")

@router.message(Command("start"))
async def cmd_start(m: Message):
    upsert_user(m.from_user.id, m.from_user.username or "")
    sub = get_sub(m.from_user.id)
    if not sub:
        till = grant_trial(m.from_user.id, TRIAL_DAYS)
        await m.answer(
            "–ü—Ä–∏–≤–µ—Ç! –í–∫–ª—é—á–∏–ª –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ç—Ä–∏–∞–ª –¥–æ {till}.\n"
            "–ö–æ–º–∞–Ω–¥—ã: /search <–º–∞–≥–∞–∑–∏–Ω>, /buy, /profile, /stores, /redeem –ö–û–î, /help".format(till=till)
        )
    else:
        await m.answer("–°–Ω–æ–≤–∞ —Ç—ã! –ü—Ä–æ–±—É–π: /search ozon")

@router.message(Command("help"))
async def cmd_help(m: Message):
    await m.answer(
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/search &lt;–º–∞–≥–∞–∑–∏–Ω&gt; ‚Äî –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–ø—Ä–∏–º–µ—Ä: /search ozon)\n"
        "/stores ‚Äî —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤\n"
        "/profile ‚Äî —Å—Ç–∞—Ç—É—Å –ø–æ–¥–ø–∏—Å–∫–∏\n"
        "/buy ‚Äî –æ—Ñ–æ—Ä–º–∏—Ç—å –ø–æ–¥–ø–∏—Å–∫—É\n"
        "/redeem &lt;–∫–æ–¥&gt; ‚Äî –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–æ–∫–æ–¥\n"
        "/reload ‚Äî –æ–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏"
    )

@router.message(Command("profile"))
async def cmd_profile(m: Message):
    sub = get_sub(m.from_user.id)
    if not sub:
        await m.answer("–°—Ç–∞—Ç—É—Å: –Ω–µ—Ç –ø–æ–¥–ø–∏—Å–∫–∏. /buy ‚Äî –æ—Ñ–æ—Ä–º–∏—Ç—å (249‚ÇΩ/–º–µ—Å)")
    else:
        await m.answer(f"–°—Ç–∞—Ç—É—Å: {sub['status']} –¥–æ {sub['until']}")

@router.message(Command("buy"))
async def cmd_buy(m: Message):
    await m.answer(
        f"–ü–æ–¥–ø–∏—Å–∫–∞ {MONTHLY_PRICE_RUB}‚ÇΩ/–º–µ—Å.\n"
        f"–ù–∞ MVP ‚Äî –ø—Ä–æ–º–æ–∫–æ–¥ –æ—Ç –∞–¥–º–∏–Ω–∞: /redeem –ö–û–î."
    )

@router.message(Command("redeem"))
async def cmd_redeem(m: Message):
    parts = m.text.split(maxsplit=1)
    if len(parts) < 2:
        return await m.answer("–§–æ—Ä–º–∞—Ç: /redeem &lt;–∫–æ–¥&gt;")
    code = parts[1].strip()
    if not code:
        return await m.answer("–ü—É—Å—Ç–æ–π –∫–æ–¥.")
    if PROMO_CODES and code not in PROMO_CODES:
        return await m.answer("–ö–æ–¥ –Ω–µ–≤–µ—Ä–Ω—ã–π.")
    until = grant_month(m.from_user.id, 1)
    await m.answer(f"–ü–æ–¥–ø–∏—Å–∫–∞ –∞–∫—Ç–∏–≤–Ω–∞ –¥–æ {until}. /profile ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å")

@router.message(Command("stores"))
async def cmd_stores(m: Message):
    # –ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤ –∏–∑ ALIASES (—É–Ω–∏–∫–∞–ª—å–Ω–æ –ø–æ slug)
    seen = set()
    slugs = []
    # –ü—Ä–æ–π–¥—ë–º—Å—è –ø–æ DEFAULT_STORES_JSON, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ
    conf = json.loads(STORES_JSON or DEFAULT_STORES_JSON)
    from_conf = [x["store"] for x in conf.get("stores", [])]
    for slug in from_conf:
        if slug not in seen:
            seen.add(slug)
            slugs.append(slug)
    if not slugs:
        return await m.answer("–ü–æ–∫–∞ –ø—É—Å—Ç–æ. –ù–∞–∂–º–∏ /reload, –∑–∞—Ç–µ–º /search ozon.")
    await m.answer("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–∞–≥–∞–∑–∏–Ω—ã:\n" + "\n".join(f"‚Ä¢ {s}" for s in slugs))

@router.message(Command("search"))
async def cmd_search(m: Message):
    try:
        args = m.text.split()[1:]
        if not args:
            return await m.answer("–§–æ—Ä–º–∞—Ç: /search &lt;–º–∞–≥–∞–∑–∏–Ω&gt;\n–ù–∞–ø—Ä–∏–º–µ—Ä: /search ozon")
        user_store = " ".join(args)
        store = normalize_store_name(user_store) or user_store.strip().lower()
        if not sub_active(m.from_user.id):
            return await m.answer("–ù—É–∂–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞—è –ø–æ–¥–ø–∏—Å–∫–∞. /buy ‚Äî –æ—Ñ–æ—Ä–º–∏—Ç—å (–µ—Å—Ç—å —Ç—Ä–∏–∞–ª –≤ /start)")
        results = search_deals(store, limit=10)
        if not results:
            # –ú—è–≥–∫–æ –∏–Ω–∏—Ü–∏–∏—Ä—É–µ–º —Å–±–æ—Ä –∏ –ø–æ–¥—Å–∫–∞–∂–µ–º
            asyncio.get_event_loop().call_later(1, lambda: asyncio.create_task(scrape_job()))
            return await m.answer("–ü–æ–∫–∞ –ø—É—Å—Ç–æ –ø–æ —ç—Ç–æ–º—É –º–∞–≥–∞–∑–∏–Ω—É. –ù–∞–∂–º–∏ /reload, –ø–æ–¥–æ–∂–¥–∏ 10‚Äì20 —Å–µ–∫ –∏ –ø–æ–≤—Ç–æ—Ä–∏ –∑–∞–ø—Ä–æ—Å.")
        for d in results:
            await m.answer(fmt_deal(d), link_preview_options=LinkPreviewOptions(is_disabled=True))
    except Exception as e:
        log.exception("[SEARCH] handler error")
        await m.answer(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e!s}")

# ======== SCHEDULER ========
scheduler: Optional[AsyncIOScheduler] = None

async def scrape_job():
    try:
        cnt = run_all_sources()
        log.info(f"[SCRAPER] added: {cnt}")
    except Exception:
        log.exception("[SCRAPER] error")

async def main():
    if not BOT_TOKEN:
        raise RuntimeError("Set BOT_TOKEN env var")
    init_db()
    bot = Bot(token=BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
    dp = Dispatcher()
    dp.include_router(router)
    global scheduler
    scheduler = AsyncIOScheduler(timezone=ZoneInfo(TIMEZONE))
    scheduler.add_job(scrape_job, "interval", minutes=20, id="scrape")  # –ø–æ—á–∞—â–µ
    scheduler.add_job(cleanup_old, "cron", hour=3, minute=0, id="cleanup")
    scheduler.start()
    loop = asyncio.get_running_loop()
    loop.call_later(5, lambda: asyncio.create_task(scrape_job()))
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
